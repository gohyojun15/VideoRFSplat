---
layout: inner
position: right
priority: 4
categories: "media"

title: "VideoRFSplat: Improved Sampling with Asynchronous Sampling"
blocks:
    - text: |
        Also, previous works that jointly generate multi-view images and camera poses often suffer from early-stage "mutual ambiguity," where each modality (pose vs. image) is noisy and can push the generation in conflicting directions. As illustrated in Fig. 3, this leads to instability such as misaligned poses or rapid oscillations in viewpoint.

        To address this, we perform asynchronous sampling, in which we decompose the timesteps for image latents and camera rays. Concretely, we denoise the camera pose more quickly (i.e., use a shorter schedule for pose updates) so that a stable viewpoint is established early on, reducing the risk of inconsistent updates to the scene (see Fig. 4 for how this schedule is arranged).

        In practice, we define two learned modules for image and pose, respectively, with a single text prompt. We train them via a joint denoising objective to preserve consistency between the evolving image and the evolving pose. Because the camera pose is denoised more aggressively, it stabilizes sooner, guiding the image generation toward more coherent multi-view samples. We further enhance this process using a strategy similar to classifier-free guidance (CFG) for the pose when it is nearly resolved, treating it as an approximately unconditional prior. This stabilizes the viewpoint while the images continue to refine.
    
    - image: "files/asynchronous_sampling.png"
---
